{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770763fc-b865-4c71-b351-7dc6f1617c90",
   "metadata": {},
   "source": [
    "# Load Basal Ganglia data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c329498",
   "metadata": {},
   "source": [
    "Update `path_base` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abae882-3338-402b-9267-248edaabb6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import functools as fct\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from tqdm import tqdm\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "\n",
    "@register_cell_magic\n",
    "def skip_if(line, cell):\n",
    "    if eval(line):\n",
    "        return\n",
    "    get_ipython().run_cell(cell)\n",
    "    \n",
    "def fetch_data(download_url : str, download_file_name : Path) -> None:\n",
    "    from urllib.request import urlopen\n",
    "    from urllib.error import HTTPError, URLError\n",
    "    import ssl\n",
    "    import shutil\n",
    "\n",
    "    if not download_url.startswith(\"https://\"):\n",
    "        raise ValueError(\"Only HTTPS URLs are allowed.\")\n",
    "\n",
    "    if download_file_name.exists():\n",
    "        print(f\"Using existing file at: {download_file_name.resolve()}\")\n",
    "        return\n",
    "\n",
    "    # Ensure parent directory exists\n",
    "    download_file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading file to: {download_file_name.resolve()}\")\n",
    "\n",
    "        context = ssl.create_default_context()\n",
    "\n",
    "        # Download the file from `url` and save it locally under `file_name`:\n",
    "        with urlopen(download_url, context=context, timeout=10) as response, download_file_name.open('wb') as download_out_file:\n",
    "            shutil.copyfileobj(response, download_out_file)\n",
    "\n",
    "        print(f\"Downloaded file to {download_file_name.resolve()}\")\n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTP error: {e.code} - {e.reason}\")\n",
    "    except URLError as e:\n",
    "        print(f\"URL error: {e.reason}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "def load_csv(url: str, cache_path: Path = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a CSV file from a URL, caching it locally.\n",
    "    If the file already exists on disk, read from disk instead of downloading.\n",
    "    \"\"\"\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    if cache_path is not None and cache_path.exists():\n",
    "        print(f\"Loading cached file: {cache_path}\")\n",
    "        return pd.read_csv(cache_path)\n",
    "\n",
    "    df_csv = pd.DataFrame()\n",
    "\n",
    "    print(f\"Downloading from {url}\")\n",
    "    with urlopen(url) as response:\n",
    "        df_csv = pd.read_csv(response)\n",
    "\n",
    "    if cache_path is not None:\n",
    "        df_csv.to_csv(cache_path)\n",
    "\n",
    "    return df_csv\n",
    "\n",
    "def extract_rank_data(rank_name, rank_color, *argv):\n",
    "    d_ait_concat_rank     = pd.concat([df.obs[rank_name] for df in argv]).astype('category')\n",
    "    d_ait_concat_rank_col = pd.concat([df.obs[rank_color] for df in argv]).astype('category')\n",
    "\n",
    "    rank_id_names = d_ait_concat_rank.cat.categories[:].to_list()\n",
    "    rank_indices = [np.where(d_ait_concat_rank == rank_id_name)[0].squeeze() for rank_id_name in rank_id_names]\n",
    "    rank_colors  = [hex_to_rgbf(d_ait_concat_rank_col.iloc[inner_list[0]]) for inner_list in rank_indices]\n",
    "\n",
    "    return rank_id_names, rank_indices, rank_colors\n",
    "\n",
    "def compute_rank_means(rank_d_ait : ad.AnnData, rank_name : str, rank_var_idx: np.ndarray) -> tuple[np.ndarray, list[str], npt.NDArray[np.object_]]:\n",
    "    import gc\n",
    "    print(f\"Load {rank_name} data for {rank_d_ait.filename}\")\n",
    "    rank_data = rank_d_ait.obs[rank_name]\n",
    "    rank_id_names = rank_data.cat.categories[:].to_list()\n",
    "    rank_indices = np.array([np.where(rank_data == rank_id_name)[0].squeeze() for rank_id_name in rank_id_names], dtype=object)\n",
    "\n",
    "    print(f\"Copy data to memory\")\n",
    "    features = rank_d_ait.X.to_memory()[:, rank_var_idx]\n",
    "\n",
    "    n_cols = features.shape[1]\n",
    "    n_rows = len(rank_indices)\n",
    "    print(f\"Reserve dense matrix: {n_rows} x {n_cols}\")\n",
    "    means = np.zeros(shape=(n_rows, n_cols), dtype=np.float32)\n",
    "\n",
    "    print(f\"Compute means...\")\n",
    "    iterator = tqdm(enumerate(rank_indices), total=n_rows, disable=False)\n",
    "    for i, rows in iterator:\n",
    "        if len(rows) == 0:\n",
    "            continue  # avoid division by zero\n",
    "        features_sub = features[rows, :]  # still sparse\n",
    "        means_row = features_sub.mean(axis=0)  # (1, n) matrix\n",
    "        means[i] = np.asarray(means_row).ravel()\n",
    "\n",
    "    print(f\"Cleaning up...\")\n",
    "    features = None\n",
    "    del features\n",
    "    gc.collect()\n",
    "\n",
    "    return means, rank_id_names, rank_indices\n",
    "\n",
    "def reverse_jagged_mapping(forward_map: npt.NDArray[np.object_]) -> npt.NDArray[np.object_]:\n",
    "    \"\"\"\n",
    "    Reverses a jagged mapping where the index is the source and the values\n",
    "    are the destinations.\n",
    "\n",
    "    Example:\n",
    "        forward_map = [[0, 1], [2, 3, 4], [5]]\n",
    "        (0->0,1), (1->2,3,4), (2->5)\n",
    "\n",
    "        Returns: [[0], [0], [1], [1], [1], [2]]\n",
    "        (0->0), (1->0), (2->1), (3->1), (4->1), (5->2)\n",
    "\n",
    "    Example code:\n",
    "        forward_mapping = np.array([\n",
    "            np.array([0, 1], dtype=np.int64),\n",
    "            np.array([2, 3, 4], dtype=np.int64),\n",
    "            np.array([5], dtype=np.int64),\n",
    "        ], dtype=object)\n",
    "\n",
    "        print(\"Forward Mapping (Jagged Array):\")\n",
    "        for i, destinations in enumerate(forward_mapping):\n",
    "            print(f\"  {i} -> {destinations}\")\n",
    "\n",
    "        # Get the reversed mapping\n",
    "        reverse_mapping = reverse_jagged_mapping(forward_mapping)\n",
    "\n",
    "        print(\"\\nReversed Mapping (1D Array):\")\n",
    "        print(reverse_mapping)\n",
    "        print(\"\\nMeaning:\")\n",
    "        for destination, source in enumerate(reverse_mapping):\n",
    "            print(f\"  {destination} -> {source}\")\n",
    "\n",
    "    Args:\n",
    "        forward_map: A NumPy array with dtype=object, where each element is\n",
    "                     a 1D NumPy array of int64 destinations.\n",
    "\n",
    "    Returns:\n",
    "        A 1D NumPy array of int64 where the index represents the destination\n",
    "        and the value represents its source.\n",
    "    \"\"\"\n",
    "    # Handle the edge case of an empty input map\n",
    "    if len(forward_map) == 0:\n",
    "        return np.array([], dtype=np.int64)\n",
    "\n",
    "    # Concatenate all destination values into a single flat array\n",
    "    # e.g., [0, 1, 2, 3, 4, 5]\n",
    "    all_destinations = np.concatenate(forward_map)\n",
    "\n",
    "    # Handle case where there are no destinations\n",
    "    if all_destinations.size == 0:\n",
    "        return np.array([], dtype=np.int64)\n",
    "\n",
    "    # Get the lengths of each inner array to know how many times to repeat the source index\n",
    "    # e.g., [2, 3, 1]\n",
    "    lengths = np.array([len(arr) for arr in forward_map])\n",
    "\n",
    "    # Create a parallel array of source indices\n",
    "    # np.repeat([0, 1, 2], [2, 3, 1]) -> [0, 0, 1, 1, 1, 2]\n",
    "    source_indices = np.repeat(np.arange(len(forward_map)), lengths)\n",
    "\n",
    "    # Create the output array. Its size is determined by the max destination value.\n",
    "    output_size = np.max(all_destinations) + 1\n",
    "    reverse_map = np.empty(output_size, dtype=np.int64)\n",
    "\n",
    "    # Populate the reverse map.\n",
    "    reverse_map[all_destinations] = source_indices\n",
    "    reverse_map = np.array([np.array([x]) for x in reverse_map], dtype=object)\n",
    "\n",
    "    return reverse_map\n",
    "\n",
    "def hex_to_rgb(hex_str : str):\n",
    "    hex_str_val = hex_str.lstrip('#')\n",
    "    return np.array([int(hex_str_val[i:i + 2], 16) for i in (0, 2, 4)], dtype=float)\n",
    "\n",
    "def hex_to_rgbf(hex_str : str):\n",
    "    return hex_to_rgb(hex_str) / 255\n",
    "\n",
    "def ensure_c_contiguous(arr: np.ndarray) -> np.ndarray:\n",
    "    return arr if arr.flags['C_CONTIGUOUS'] else np.ascontiguousarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b910f7f5-0a38-4e9e-b8c6-a779a7929c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to store the data\n",
    "path_base = Path(\"E:/avieth/Data/Allen/Basal/\")\n",
    "\n",
    "# Settings\n",
    "use_cluster_mean = True # Whether to add a dataset with means of all shared vars per cluster (requires lots of memory to compute)\n",
    "use_cache = True        # Cache values that require lots of memory of compute time in path_base\n",
    "use_prep_umap = True    # Whether to download a precomputed UMAP\n",
    "\n",
    "# Remote data, see https://alleninstitute.github.io/HMBA_BasalGanglia_Consensus_Taxonomy/\n",
    "url_base = \"https://released-taxonomies-802451596237-us-west-2.s3.us-west-2.amazonaws.com/HMBA/BasalGanglia\"\n",
    "data_version = \"BICAN_05072025_pre-print_release\"\n",
    "\n",
    "HUMAN = 0\n",
    "MACAQ = 1\n",
    "MARMO = 2\n",
    "SPECIES = [HUMAN, MACAQ, MARMO]\n",
    "\n",
    "data_names = [\"\"] * 3\n",
    "data_names[HUMAN] = \"Human_HMBA_basalganglia_AIT_pre-print.h5ad\"\n",
    "data_names[MACAQ] = \"Macaque_HMBA_basalganglia_AIT_pre-print.h5ad\"\n",
    "data_names[MARMO] = \"Marmoset_HMBA_basalganglia_AIT_pre-print.h5ad\"\n",
    "\n",
    "data_ait_urls = [f\"{url_base}/{data_version}/{data_names[kind]}\" for kind in SPECIES]\n",
    "data_ait_paths = [path_base / data_names[kind] for kind in SPECIES]\n",
    "\n",
    "print(\"Ensure data is available locally...\")\n",
    "\n",
    "# Download data\n",
    "for kind in SPECIES:\n",
    "    fetch_data(data_ait_urls[kind], data_ait_paths[kind])\n",
    "\n",
    "print(\"Data is available locally. Loading data...\")\n",
    "\n",
    "# Load anndata [you'll need a good amount of RAM even though not all data is loaded to memory]\n",
    "d_ait = [ad.io.read_h5ad(data_ait_paths[kind], backed='r') for kind in SPECIES]\n",
    "\n",
    "# expected:\n",
    "# (1'034'819, 36'601)\n",
    "# (  548'281, 35'219)\n",
    "# (  313'033, 35'787)\n",
    "for kind in SPECIES:\n",
    "    print(f\"({d_ait[kind].n_obs:,}, {d_ait[kind].n_vars:,})\".replace(\",\", \"'\").rjust(19))\n",
    "\n",
    "print(\"Finished loading data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ba1ca-473a-4b3f-a2c8-1b6acdd715da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep some meta data\n",
    "print(f\"Compute shared variable names\")\n",
    "shared_var_names = fct.reduce(np.intersect1d, [d_ait[kind].var_names.to_numpy() for kind in SPECIES])\n",
    "shared_var_names = np.array(np.sort(shared_var_names), dtype=np.dtypes.StringDType())\n",
    "\n",
    "all_obs_names    = np.concatenate([d_ait[kind].obs_names.to_numpy() for kind in SPECIES])\n",
    "shared_obs_names = fct.reduce(np.intersect1d, all_obs_names)\n",
    "\n",
    "assert len(shared_obs_names) == 0\n",
    "\n",
    "species_indices_starts = [\n",
    "    0,\n",
    "    d_ait[HUMAN].n_obs,\n",
    "    d_ait[HUMAN].n_obs + d_ait[MACAQ].n_obs\n",
    "]\n",
    "species_indices_ends   = [\n",
    "    d_ait[HUMAN].n_obs,\n",
    "    d_ait[HUMAN].n_obs + d_ait[MACAQ].n_obs,\n",
    "    d_ait[HUMAN].n_obs + d_ait[MACAQ].n_obs + d_ait[MARMO].n_obs\n",
    "]\n",
    "\n",
    "print(f\"Compute indices\")\n",
    "var_indices = [np.array([d_ait[kind].var_names.get_loc(var) for var in shared_var_names if var in d_ait[kind].var_names]) for kind in SPECIES]\n",
    "\n",
    "print(f\"Num combined observations: {species_indices_ends[-1]}\")\n",
    "print(f\"Num shared variables: {shared_var_names.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd9b755-d474-4298-8ff3-629a088a5658",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not use_cluster_mean\n",
    "# Mean feature for each cluster\n",
    "print(f\"Get cluster means\")\n",
    "\n",
    "def check_cluster_cache_exists(cluster_path_base : Path) -> bool :\n",
    "    return (\n",
    "        (cluster_path_base / \"cluster_means_concat.npy\").is_file()\n",
    "        and (cluster_path_base / \"cluster_names_concat.npy\").is_file()\n",
    "        and (cluster_path_base / \"cluster_indices_concat.npy\").is_file()\n",
    "        and (cluster_path_base / \"reverse_cluster_map.npy\").is_file()\n",
    "    )\n",
    "\n",
    "def save_cluster_cache(cluster_path_base : Path, cluster_means_concat_cache, cluster_names_concat_cache, cluster_indices_concat_cache, reverse_cluster_map_cache) -> None:\n",
    "    np.save(cluster_path_base / \"cluster_means_concat.npy\", cluster_means_concat_cache, allow_pickle=True)\n",
    "    np.save(cluster_path_base / \"cluster_names_concat.npy\", cluster_names_concat_cache, allow_pickle=True)\n",
    "    np.save(cluster_path_base / \"cluster_indices_concat.npy\", cluster_indices_concat_cache, allow_pickle=True)\n",
    "    np.save(cluster_path_base / \"reverse_cluster_map.npy\", reverse_cluster_map_cache, allow_pickle=True)\n",
    "\n",
    "def load_cluster_cache(cluster_path_base : Path):\n",
    "    cluster_means_concat_cache    = np.load(cluster_path_base/ \"cluster_means_concat.npy\", allow_pickle=True)\n",
    "    cluster_names_concat_cache    = np.load(cluster_path_base/ \"cluster_names_concat.npy\", allow_pickle=True)\n",
    "    cluster_indices_concat_cache  = np.load(cluster_path_base/ \"cluster_indices_concat.npy\", allow_pickle=True)\n",
    "    reverse_cluster_map_cache     = np.load(cluster_path_base/ \"reverse_cluster_map.npy\", allow_pickle=True)\n",
    "    return cluster_means_concat_cache, cluster_names_concat_cache, cluster_indices_concat_cache, reverse_cluster_map_cache\n",
    "\n",
    "cluster_cache_exists = check_cluster_cache_exists(path_base)\n",
    "\n",
    "if cluster_cache_exists and use_cache:\n",
    "    print(\"Load cluster means from cache\")\n",
    "    cluster_means_concat, cluster_names_concat, cluster_indices_concat, reverse_cluster_map = load_cluster_cache(path_base)\n",
    "else:\n",
    "    print(\"Compute cluster means\")\n",
    "    cluster_means_human, cluster_names_human, cluster_indices_human = compute_rank_means(d_ait[HUMAN], \"Cluster\", var_indices[HUMAN])\n",
    "    cluster_means_macaq, cluster_names_macaq, cluster_indices_macaq = compute_rank_means(d_ait[MACAQ], \"Cluster\", var_indices[MACAQ])\n",
    "    cluster_means_marmo, cluster_names_marmo, cluster_indices_marmo = compute_rank_means(d_ait[MARMO], \"Cluster\", var_indices[MARMO])\n",
    "    \n",
    "    # Concatenate features data\n",
    "    cluster_means_concat   = np.concatenate([cluster_means_human, cluster_means_macaq, cluster_means_marmo], axis=0)\n",
    "    cluster_names_concat   = [name for cluster in [cluster_names_human, cluster_names_macaq, cluster_names_marmo] for name in cluster]\n",
    "    cluster_indices_concat = np.concatenate([\n",
    "        cluster_indices_human + species_indices_starts[HUMAN],\n",
    "        cluster_indices_macaq + species_indices_starts[MACAQ],\n",
    "        cluster_indices_marmo + species_indices_starts[MARMO]], \n",
    "        axis=0)\n",
    "    \n",
    "    # Reverse the mapping, from data to clusters\n",
    "    reverse_cluster_map = reverse_jagged_mapping(cluster_indices_concat)\n",
    "\n",
    "    if use_cache:\n",
    "        print(\"Save cluster means to cache\")\n",
    "        save_cluster_cache(path_base, cluster_means_concat, cluster_names_concat, cluster_indices_concat, reverse_cluster_map)\n",
    "\n",
    "assert reverse_cluster_map.shape[0] == species_indices_ends[-1]\n",
    "\n",
    "print(f\"Num combined clusters: {cluster_means_concat.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233c78eb-0191-401c-a8eb-e07cb08670c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Concatenated scVI features\")\n",
    "# Extract scVI feature matrix (dense, 64 variables expected)\n",
    "d_ait_scVI = [d_ait[kind].obsm['X_scVI'] for kind in SPECIES]\n",
    "\n",
    "# Concatenate scVI features\n",
    "d_ait_concat_scVI = np.concatenate(d_ait_scVI, axis=0)\n",
    "#d_ait_concat_scVI = np.concatenate((d_ait_marmo_scVI, d_ait_marmo_scVI), axis=0)\n",
    "\n",
    "print(f\"Concatenated size: {d_ait_concat_scVI.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b56d77-e09a-4b0c-8ddc-461cdf992934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and concatenate classification\n",
    "print(\"Extract rank data\")\n",
    "neigh_names,    neigh_indices,    neigh_colors    = extract_rank_data('Neighborhood', 'color_hex_neighborhood', d_ait[HUMAN], d_ait[MACAQ], d_ait[MARMO])\n",
    "class_names,    class_indices,    class_colors    = extract_rank_data('Class', 'color_hex_class', d_ait[HUMAN], d_ait[MACAQ], d_ait[MARMO])\n",
    "subclass_names, subclass_indices, subclass_colors = extract_rank_data('Subclass', 'color_hex_subclass', d_ait[HUMAN], d_ait[MACAQ], d_ait[MARMO])\n",
    "group_names,    group_indices,    group_colors    = extract_rank_data('Group', 'color_hex_group', d_ait[HUMAN], d_ait[MACAQ], d_ait[MARMO])\n",
    "\n",
    "species_names   = ['Human', 'Macaque', 'Marmoset'] # human, macaque, marmoset\n",
    "species_colors  = [hex_to_rgbf('1b6097'), hex_to_rgbf('318e2d'), hex_to_rgbf('db423f')] # human, macaque, marmoset\n",
    "species_indices = [\n",
    "    np.arange(species_indices_starts[0], species_indices_ends[0]),\n",
    "    np.arange(species_indices_starts[1], species_indices_ends[1]),\n",
    "    np.arange(species_indices_starts[2], species_indices_ends[2])\n",
    "]\n",
    "\n",
    "print(\"Finished extracting rank data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ee6d6-a0c3-43f3-90e4-208c3de5aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip_if not use_prep_umap\n",
    "# Download pre-computed UMAP\n",
    "print(\"Load pre-computed UMAP\")\n",
    "\n",
    "# Download / use cache\n",
    "umap_url = \"https://allen-brain-cell-atlas.s3-us-west-2.amazonaws.com/metadata/HMBA-BG-taxonomy-CCN20250428/20250531/cell_2d_embedding_coordinates.csv\"\n",
    "umap_cache_file = path_base / \"cell_2d_embedding_coordinates.csv\"\n",
    "\n",
    "# Extract coordinates from data frame\n",
    "d_umap = load_csv(umap_url, umap_cache_file)\n",
    "umap_coords = d_umap[[\"x\", \"y\"]].to_numpy(dtype=\"float32\")\n",
    "\n",
    "umap_coords = ensure_c_contiguous(umap_coords)\n",
    "\n",
    "# Check names \n",
    "umap_labels = d_umap[[\"cell_label\"]].to_numpy().ravel()\n",
    "assert np.array_equal(umap_labels, all_obs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a4efb-ce9b-4988-b0e7-c41f0cee076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mvstudio.data\n",
    "dh = mvstudio.data.Hierarchy()\n",
    "\n",
    "print(\"Add points to ManiVault\")\n",
    "mv_concat_scVI = dh.addPointsItem(d_ait_concat_scVI, \"scVI\")\n",
    "\n",
    "print(\"Add clusters to ManiVault\")\n",
    "mv_concat_species  = dh.addClusterItem(mv_concat_scVI.datasetId, species_indices, \"Species\", names=species_names, colors=species_colors)\n",
    "mv_concat_neigh    = dh.addClusterItem(mv_concat_scVI.datasetId, neigh_indices, \"Neighbors\", names=neigh_names, colors=neigh_colors)\n",
    "mv_concat_class    = dh.addClusterItem(mv_concat_scVI.datasetId, class_indices, \"Class\", names=class_names, colors=class_colors)\n",
    "mv_concat_subclass = dh.addClusterItem(mv_concat_scVI.datasetId, subclass_indices, \"Subclass\", names=subclass_names, colors=subclass_colors)\n",
    "mv_concat_group    = dh.addClusterItem(mv_concat_scVI.datasetId, group_indices, \"Group\", names=group_names, colors=group_colors)\n",
    "\n",
    "if use_cluster_mean:\n",
    "    print(\"Add aggregated feature data to ManiVault\")\n",
    "    mv_concat_clusterM = dh.addPointsItem(cluster_means_concat, \"Cluster expression (mean)\", mv_concat_scVI.datasetId, shared_var_names)\n",
    "\n",
    "    # Only link in one direction. Adding both mappings would cause an unfortunate back-and-forth selection loop\n",
    "    #success_link = mv_concat_clusterM.setLinkedData(mv_concat_scVI, cluster_indices_concat)\n",
    "    success_link = mv_concat_scVI.setLinkedData(mv_concat_clusterM, reverse_cluster_map)\n",
    "    assert success_link\n",
    "\n",
    "if use_prep_umap:\n",
    "    print(\"Add pre-computed UMAP to ManiVault\")\n",
    "    mv_concat_scVI = dh.addDerivedPointsItem(umap_coords, \"UMAP embedding\", mv_concat_scVI.datasetId, [\"UMAP x\", \"UMAP, y\"])\n",
    "\n",
    "print(\"Done adding data to ManiVault\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ManiVaultStudio",
   "language": "python",
   "name": "manivaultstudio"
  },
  "language_info": {
   "codemirror_mode": "",
   "file_extension": "py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "",
   "pygments_lexer": "",
   "version": "3.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
