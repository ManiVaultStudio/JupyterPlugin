{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770763fc-b865-4c71-b351-7dc6f1617c90",
   "metadata": {},
   "source": [
    "# Load Basal Ganglia data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c329498",
   "metadata": {},
   "source": [
    "Update `path_base` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abae882-3338-402b-9267-248edaabb6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import functools as fct\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from tqdm import tqdm\n",
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "\n",
    "@register_cell_magic\n",
    "def skip_if(line, cell):\n",
    "    if eval(line):\n",
    "        return\n",
    "    get_ipython().run_cell(cell)\n",
    "    \n",
    "\n",
    "def fetch_data(download_url : str, download_file_name : Path) -> None:\n",
    "    from urllib.request import urlopen\n",
    "    from urllib.error import HTTPError, URLError\n",
    "    import ssl\n",
    "    import shutil\n",
    "\n",
    "    if not download_url.startswith(\"https://\"):\n",
    "        raise ValueError(\"Only HTTPS URLs are allowed.\")\n",
    "\n",
    "    if download_file_name.exists():\n",
    "        print(f\"Using existing file at: {download_file_name.resolve()}\")\n",
    "        return\n",
    "\n",
    "    # Ensure parent directory exists\n",
    "    download_file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        print(f\"Downloading file to: {download_file_name.resolve()}\")\n",
    "\n",
    "        context = ssl.create_default_context()\n",
    "\n",
    "        # Download the file from `url` and save it locally under `file_name`:\n",
    "        with urlopen(download_url, context=context, timeout=10) as response, download_file_name.open('wb') as download_out_file:\n",
    "            shutil.copyfileobj(response, download_out_file)\n",
    "\n",
    "        print(f\"Downloaded file to {download_file_name.resolve()}\")\n",
    "    except HTTPError as e:\n",
    "        print(f\"HTTP error: {e.code} - {e.reason}\")\n",
    "    except URLError as e:\n",
    "        print(f\"URL error: {e.reason}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "def extract_rank_data(rank_name, rank_color, *argv):\n",
    "    d_ait_concat_rank     = pd.concat([df.obs[rank_name] for df in argv]).astype('category')\n",
    "    d_ait_concat_rank_col = pd.concat([df.obs[rank_color] for df in argv]).astype('category')\n",
    "\n",
    "    rank_id_names = d_ait_concat_rank.cat.categories[:].to_list()\n",
    "    rank_indices = [np.where(d_ait_concat_rank == rank_id_name)[0].squeeze() for rank_id_name in rank_id_names]\n",
    "    rank_colors  = [hex_to_rgbf(d_ait_concat_rank_col.iloc[inner_list[0]]) for inner_list in rank_indices]\n",
    "\n",
    "    return rank_id_names, rank_indices, rank_colors\n",
    "\n",
    "\n",
    "def compute_rank_means(rank_d_ait : ad.AnnData, rank_name : str, rank_var_idx: np.ndarray) -> tuple[np.ndarray, list[str], npt.NDArray[np.object_]]:\n",
    "    import gc\n",
    "    print(f\"Load {rank_name} data for {rank_d_ait.filename}\")\n",
    "    rank_data = rank_d_ait.obs[rank_name]\n",
    "    rank_id_names = rank_data.cat.categories[:].to_list()\n",
    "    rank_indices = np.array([np.where(rank_data == rank_id_name)[0].squeeze() for rank_id_name in rank_id_names], dtype=object)\n",
    "\n",
    "    print(f\"Copy data to memory\")\n",
    "    features = rank_d_ait.X.to_memory()[:, rank_var_idx]\n",
    "\n",
    "    n_cols = features.shape[1]\n",
    "    n_rows = len(rank_indices)\n",
    "    print(f\"Reserve dense matrix: {n_rows} x {n_cols}\")\n",
    "    means = np.zeros(shape=(n_rows, n_cols), dtype=np.float32)\n",
    "\n",
    "    print(f\"Compute means...\")\n",
    "    iterator = tqdm(enumerate(rank_indices), total=n_rows, disable=False)\n",
    "    for i, rows in iterator:\n",
    "        if len(rows) == 0:\n",
    "            continue  # avoid division by zero\n",
    "        features_sub = features[rows, :]  # still sparse\n",
    "        means_row = features_sub.mean(axis=0)  # (1, n) matrix\n",
    "        means[i] = np.asarray(means_row).ravel()\n",
    "\n",
    "    print(f\"Cleaning up...\")\n",
    "    features = None\n",
    "    del features\n",
    "    gc.collect()\n",
    "\n",
    "    return means, rank_id_names, rank_indices\n",
    "\n",
    "\n",
    "def reverse_jagged_mapping(forward_map: npt.NDArray[np.object_]) -> npt.NDArray[np.object_]:\n",
    "    \"\"\"\n",
    "    Reverses a jagged mapping where the index is the source and the values\n",
    "    are the destinations.\n",
    "\n",
    "    Example:\n",
    "        forward_map = [[0, 1], [2, 3, 4], [5]]\n",
    "        (0->0,1), (1->2,3,4), (2->5)\n",
    "\n",
    "        Returns: [[0], [0], [1], [1], [1], [2]]\n",
    "        (0->0), (1->0), (2->1), (3->1), (4->1), (5->2)\n",
    "\n",
    "    Example code:\n",
    "        forward_mapping = np.array([\n",
    "            np.array([0, 1], dtype=np.int64),\n",
    "            np.array([2, 3, 4], dtype=np.int64),\n",
    "            np.array([5], dtype=np.int64),\n",
    "        ], dtype=object)\n",
    "\n",
    "        print(\"Forward Mapping (Jagged Array):\")\n",
    "        for i, destinations in enumerate(forward_mapping):\n",
    "            print(f\"  {i} -> {destinations}\")\n",
    "\n",
    "        # Get the reversed mapping\n",
    "        reverse_mapping = reverse_jagged_mapping(forward_mapping)\n",
    "\n",
    "        print(\"\\nReversed Mapping (1D Array):\")\n",
    "        print(reverse_mapping)\n",
    "        print(\"\\nMeaning:\")\n",
    "        for destination, source in enumerate(reverse_mapping):\n",
    "            print(f\"  {destination} -> {source}\")\n",
    "\n",
    "    Args:\n",
    "        forward_map: A NumPy array with dtype=object, where each element is\n",
    "                     a 1D NumPy array of int64 destinations.\n",
    "\n",
    "    Returns:\n",
    "        A 1D NumPy array of int64 where the index represents the destination\n",
    "        and the value represents its source.\n",
    "    \"\"\"\n",
    "    # Handle the edge case of an empty input map\n",
    "    if len(forward_map) == 0:\n",
    "        return np.array([], dtype=np.int64)\n",
    "\n",
    "    # Concatenate all destination values into a single flat array\n",
    "    # e.g., [0, 1, 2, 3, 4, 5]\n",
    "    all_destinations = np.concatenate(forward_map)\n",
    "\n",
    "    # Handle case where there are no destinations\n",
    "    if all_destinations.size == 0:\n",
    "        return np.array([], dtype=np.int64)\n",
    "\n",
    "    # Get the lengths of each inner array to know how many times to repeat the source index\n",
    "    # e.g., [2, 3, 1]\n",
    "    lengths = np.array([len(arr) for arr in forward_map])\n",
    "\n",
    "    # Create a parallel array of source indices\n",
    "    # np.repeat([0, 1, 2], [2, 3, 1]) -> [0, 0, 1, 1, 1, 2]\n",
    "    source_indices = np.repeat(np.arange(len(forward_map)), lengths)\n",
    "\n",
    "    # Create the output array. Its size is determined by the max destination value.\n",
    "    output_size = np.max(all_destinations) + 1\n",
    "    reverse_map = np.empty(output_size, dtype=np.int64)\n",
    "\n",
    "    # Populate the reverse map.\n",
    "    reverse_map[all_destinations] = source_indices\n",
    "    reverse_map = np.array([np.array([x]) for x in reverse_map], dtype=object)\n",
    "\n",
    "    return reverse_map\n",
    "\n",
    "\n",
    "def hex_to_rgb(hex_str : str):\n",
    "    hex_str_val = hex_str.lstrip('#')\n",
    "    return np.array([int(hex_str_val[i:i + 2], 16) for i in (0, 2, 4)], dtype=float)\n",
    "\n",
    "def hex_to_rgbf(hex_str : str):\n",
    "    return hex_to_rgb(hex_str) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b910f7f5-0a38-4e9e-b8c6-a779a7929c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensure data is available locally...\n",
      "Using existing file at: E:\\avieth\\Data\\Allen\\Basal\\Human_HMBA_basalganglia_AIT_pre-print.h5ad\n",
      "Using existing file at: E:\\avieth\\Data\\Allen\\Basal\\Macaque_HMBA_basalganglia_AIT_pre-print.h5ad\n",
      "Using existing file at: E:\\avieth\\Data\\Allen\\Basal\\Marmoset_HMBA_basalganglia_AIT_pre-print.h5ad\n",
      "Data is available locally. Loading data...\n",
      "(1'034'819' 36'601)\n",
      "  (548'281' 35'219)\n",
      "  (313'033' 35'787)\n",
      "Finished loading data.\n"
     ]
    }
   ],
   "source": [
    "# Where to store the data\n",
    "path_base = Path().resolve()    # CHANGE ME!\n",
    "\n",
    "# Settings\n",
    "use_cluster_mean = True # Whether to add a dataset with means of all shared vars per cluster (requires lots of memory to compute)\n",
    "use_cache = True        # Cache values that require lots of memory of compute time in path_base\n",
    "\n",
    "# Remote data, see https://alleninstitute.github.io/HMBA_BasalGanglia_Consensus_Taxonomy/\n",
    "url_base = \"https://released-taxonomies-802451596237-us-west-2.s3.us-west-2.amazonaws.com/HMBA/BasalGanglia\"\n",
    "data_version = \"BICAN_05072025_pre-print_release\"\n",
    "\n",
    "HUMAN = 0\n",
    "MACAQ = 1\n",
    "MARMO = 2\n",
    "SPECIES = [HUMAN, MACAQ, MARMO]\n",
    "\n",
    "data_names = [\"\"] * 3\n",
    "data_names[HUMAN] = \"Human_HMBA_basalganglia_AIT_pre-print.h5ad\"\n",
    "data_names[MACAQ] = \"Macaque_HMBA_basalganglia_AIT_pre-print.h5ad\"\n",
    "data_names[MARMO] = \"Marmoset_HMBA_basalganglia_AIT_pre-print.h5ad\"\n",
    "\n",
    "data_ait_urls = [f\"{url_base}/{data_version}/{data_names[kind]}\" for kind in SPECIES]\n",
    "data_ait_paths = [path_base / data_names[kind] for kind in SPECIES]\n",
    "\n",
    "print(\"Ensure data is available locally...\")\n",
    "\n",
    "# Download data\n",
    "for kind in SPECIES:\n",
    "    fetch_data(data_ait_urls[kind], data_ait_paths[kind])\n",
    "\n",
    "print(\"Data is available locally. Loading data...\")\n",
    "\n",
    "# Load anndata [you'll need a good amount of RAM even though not all data is loaded to memory]\n",
    "d_ait = [ad.io.read_h5ad(data_ait_paths[kind], backed='r') for kind in SPECIES]\n",
    "\n",
    "# expected:\n",
    "# (1'034'819, 36'601)\n",
    "# (  548'281, 35'219)\n",
    "# (  313'033, 35'787)\n",
    "for kind in SPECIES:\n",
    "    print(f\"({d_ait[kind].n_obs:,}, {d_ait[kind].n_vars:,})\".replace(\",\", \"'\").rjust(19))\n",
    "\n",
    "print(\"Finished loading data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad2ba1ca-473a-4b3f-a2c8-1b6acdd715da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute shared variable names\n",
      "Compute indices\n",
      "Num combined observations: 1896133\n",
      "Num shared variables: 16140\n"
     ]
    }
   ],
   "source": [
    "# Prep some meta data\n",
    "print(f\"Compute shared variable names\")\n",
    "shared_var_names = fct.reduce(np.intersect1d, [d_ait[kind].var_names.to_numpy() for kind in SPECIES])\n",
    "shared_var_names = np.array(np.sort(shared_var_names), dtype=np.dtypes.StringDType())\n",
    "\n",
    "all_obs_names    = np.concatenate([d_ait[kind].obs_names.to_numpy() for kind in SPECIES])\n",
    "shared_obs_names = fct.reduce(np.intersect1d, all_obs_names)\n",
    "\n",
    "assert len(shared_obs_names) == 0\n",
    "\n",
    "species_indices_starts = [\n",
    "    0,\n",
    "    d_ait[HUMAN].n_obs,\n",
    "    d_ait[HUMAN].n_obs + d_ait[MACAQ].n_obs\n",
    "]\n",
    "species_indices_ends   = [\n",
    "    d_ait[HUMAN].n_obs,\n",
    "    d_ait[HUMAN].n_obs + d_ait[MACAQ].n_obs,\n",
    "    d_ait[HUMAN].n_obs + d_ait[MACAQ].n_obs + d_ait[MARMO].n_obs\n",
    "]\n",
    "\n",
    "print(f\"Compute indices\")\n",
    "var_indices = [np.array([d_ait[kind].var_names.get_loc(var) for var in shared_var_names if var in d_ait[kind].var_names]) for kind in SPECIES]\n",
    "\n",
    "print(f\"Num combined observations: {species_indices_ends[-1]}\")\n",
    "print(f\"Num shared variables: {shared_var_names.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdd9b755-d474-4298-8ff3-629a088a5658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get cluster means\n",
      "Load cluster means from cache\n",
      "Num combined clusters: 1435\n"
     ]
    }
   ],
   "source": [
    "%%skip_if not use_cluster_mean\n",
    "# Mean feature for each cluster\n",
    "print(f\"Get cluster means\")\n",
    "\n",
    "def check_cluster_cache_exists(cluster_path_base : Path) -> bool :\n",
    "    return (\n",
    "        (cluster_path_base / \"cluster_means_concat.npy\").is_file()\n",
    "        and (cluster_path_base / \"cluster_names_concat.npy\").is_file()\n",
    "        and (cluster_path_base / \"cluster_indices_concat.npy\").is_file()\n",
    "        and (cluster_path_base / \"reverse_cluster_map.npy\").is_file()\n",
    "    )\n",
    "\n",
    "def save_cluster_cache(cluster_path_base : Path, cluster_means_concat_cache, cluster_names_concat_cache, cluster_indices_concat_cache, reverse_cluster_map_cache) -> None:\n",
    "    np.save(cluster_path_base / \"cluster_means_concat.npy\", cluster_means_concat_cache, allow_pickle=True)\n",
    "    np.save(cluster_path_base / \"cluster_names_concat.npy\", cluster_names_concat_cache, allow_pickle=True)\n",
    "    np.save(cluster_path_base / \"cluster_indices_concat.npy\", cluster_indices_concat_cache, allow_pickle=True)\n",
    "    np.save(cluster_path_base / \"reverse_cluster_map.npy\", reverse_cluster_map_cache, allow_pickle=True)\n",
    "\n",
    "def load_cluster_cache(cluster_path_base : Path):\n",
    "    cluster_means_concat_cache    = np.load(cluster_path_base/ \"cluster_means_concat.npy\", allow_pickle=True)\n",
    "    cluster_names_concat_cache    = np.load(cluster_path_base/ \"cluster_names_concat.npy\", allow_pickle=True)\n",
    "    cluster_indices_concat_cache  = np.load(cluster_path_base/ \"cluster_indices_concat.npy\", allow_pickle=True)\n",
    "    reverse_cluster_map_cache     = np.load(cluster_path_base/ \"reverse_cluster_map.npy\", allow_pickle=True)\n",
    "    return cluster_means_concat_cache, cluster_names_concat_cache, cluster_indices_concat_cache, reverse_cluster_map_cache\n",
    "\n",
    "cluster_cache_exists = check_cluster_cache_exists(path_base)\n",
    "\n",
    "if cluster_cache_exists and use_cache:\n",
    "    print(\"Load cluster means from cache\")\n",
    "    cluster_means_concat, cluster_names_concat, cluster_indices_concat, reverse_cluster_map = load_cluster_cache(path_base)\n",
    "else:\n",
    "    print(\"Compute cluster means\")\n",
    "    cluster_means_human, cluster_names_human, cluster_indices_human = compute_rank_means(d_ait[HUMAN], \"Cluster\", var_indices[HUMAN])\n",
    "    cluster_means_macaq, cluster_names_macaq, cluster_indices_macaq = compute_rank_means(d_ait[MACAQ], \"Cluster\", var_indices[MACAQ])\n",
    "    cluster_means_marmo, cluster_names_marmo, cluster_indices_marmo = compute_rank_means(d_ait[MARMO], \"Cluster\", var_indices[MARMO])\n",
    "    \n",
    "    # Concatenate features data\n",
    "    cluster_means_concat   = np.concatenate([cluster_means_human, cluster_means_macaq, cluster_means_marmo], axis=0)\n",
    "    cluster_names_concat   = [name for cluster in [cluster_names_human, cluster_names_macaq, cluster_names_marmo] for name in cluster]\n",
    "    cluster_indices_concat = np.concatenate([\n",
    "        cluster_indices_human + species_indices_starts[HUMAN],\n",
    "        cluster_indices_macaq + species_indices_starts[MACAQ],\n",
    "        cluster_indices_marmo + species_indices_starts[MARMO]], \n",
    "        axis=0)\n",
    "    \n",
    "    # Reverse the mapping, from data to clusters\n",
    "    reverse_cluster_map = reverse_jagged_mapping(cluster_indices_concat)\n",
    "\n",
    "    if use_cache:\n",
    "        print(\"Save cluster means to cache\")\n",
    "        save_cluster_cache(path_base, cluster_means_concat, cluster_names_concat, cluster_indices_concat, reverse_cluster_map)\n",
    "\n",
    "assert reverse_cluster_map.shape[0] == species_indices_ends[-1]\n",
    "\n",
    "print(f\"Num combined clusters: {cluster_means_concat.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "233c78eb-0191-401c-a8eb-e07cb08670c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated scVI features\n",
      "Concatenated size: (1896133, 64)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Concatenated scVI features\")\n",
    "# Extract scVI feature matrix (dense, 64 variables expected)\n",
    "d_ait_scVI = [d_ait[kind].obsm['X_scVI'] for kind in SPECIES]\n",
    "\n",
    "# Concatenate scVI features\n",
    "d_ait_concat_scVI = np.concatenate(d_ait_scVI, axis=0)\n",
    "#d_ait_concat_scVI = np.concatenate((d_ait_marmo_scVI, d_ait_marmo_scVI), axis=0)\n",
    "\n",
    "print(f\"Concatenated size: {d_ait_concat_scVI.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0b56d77-e09a-4b0c-8ddc-461cdf992934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract rank data\n",
      "Finished extracting rank data\n"
     ]
    }
   ],
   "source": [
    "# Extract and concatenate classification\n",
    "print(\"Extract rank data\")\n",
    "neigh_names,    neigh_indices,    neigh_colors    = extract_rank_data('Neighborhood', 'color_hex_neighborhood', d_ait[HUMAN], d_ait[MACAQ], d_ait[MARMO])\n",
    "class_names,    class_indices,    class_colors    = extract_rank_data('Class', 'color_hex_class', d_ait[HUMAN], d_ait[MACAQ], d_ait[MARMO])\n",
    "subclass_names, subclass_indices, subclass_colors = extract_rank_data('Subclass', 'color_hex_subclass', d_ait[HUMAN], d_ait[MACAQ], d_ait[MARMO])\n",
    "group_names,    group_indices,    group_colors    = extract_rank_data('Group', 'color_hex_group', d_ait[HUMAN], d_ait[MACAQ], d_ait[MARMO])\n",
    "\n",
    "species_names   = ['Human', 'Macaque', 'Marmoset'] # human, macaque, marmoset\n",
    "species_colors  = [hex_to_rgbf('1b6097'), hex_to_rgbf('318e2d'), hex_to_rgbf('db423f')] # human, macaque, marmoset\n",
    "species_indices = [\n",
    "    np.arange(species_indices_starts[0], species_indices_ends[0]),\n",
    "    np.arange(species_indices_starts[1], species_indices_ends[1]),\n",
    "    np.arange(species_indices_starts[2], species_indices_ends[2])\n",
    "]\n",
    "\n",
    "print(\"Finished extracting rank data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13a4efb-ce9b-4988-b0e7-c41f0cee076b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add points to ManiVault\n",
      "The numpy array is a view. Creating a local copy to handle striding correctly...\n",
      "Add clusters to ManiVault\n",
      "Add aggregated feature data to ManiVault\n",
      "The numpy array is a view. Creating a local copy to handle striding correctly...\n",
      "Done adding data to ManiVault\n"
     ]
    }
   ],
   "source": [
    "import mvstudio.data\n",
    "dh = mvstudio.data.Hierarchy()\n",
    "\n",
    "print(\"Add points to ManiVault\")\n",
    "mv_concat_scVI = dh.addPointsItem(d_ait_concat_scVI, \"scVI\")\n",
    "\n",
    "print(\"Add clusters to ManiVault\")\n",
    "mv_concat_species  = dh.addClusterItem(mv_concat_scVI.datasetId, species_indices, \"Species\", names=species_names, colors=species_colors)\n",
    "mv_concat_neigh    = dh.addClusterItem(mv_concat_scVI.datasetId, neigh_indices, \"Neighbors\", names=neigh_names, colors=neigh_colors)\n",
    "mv_concat_class    = dh.addClusterItem(mv_concat_scVI.datasetId, class_indices, \"Class\", names=class_names, colors=class_colors)\n",
    "mv_concat_subclass = dh.addClusterItem(mv_concat_scVI.datasetId, subclass_indices, \"Subclass\", names=subclass_names, colors=subclass_colors)\n",
    "mv_concat_group    = dh.addClusterItem(mv_concat_scVI.datasetId, group_indices, \"Group\", names=group_names, colors=group_colors)\n",
    "\n",
    "if use_cluster_mean:\n",
    "    print(\"Add aggregated feature data to ManiVault\")\n",
    "    mv_concat_clusterM = dh.addPointsItem(cluster_means_concat, \"Cluster expression (mean)\", mv_concat_scVI.datasetId, shared_var_names)\n",
    "\n",
    "    # Only link in one direction. Adding both mappings would cause an unfortunate back-and-forth selection loop\n",
    "    #success_link = mv_concat_clusterM.setLinkedData(mv_concat_scVI, cluster_indices_concat)\n",
    "    success_link = mv_concat_scVI.setLinkedData(mv_concat_clusterM, reverse_cluster_map)\n",
    "    assert success_link\n",
    "\n",
    "print(\"Done adding data to ManiVault\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ManiVaultStudio",
   "language": "python",
   "name": "manivaultstudio"
  },
  "language_info": {
   "codemirror_mode": "",
   "file_extension": "py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "",
   "pygments_lexer": "",
   "version": "3.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
